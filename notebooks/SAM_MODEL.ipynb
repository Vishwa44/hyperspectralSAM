{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89920070-c7c6-4550-bd95-e01a299e6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    SamVisionConfig,\n",
    "    SamPromptEncoderConfig,\n",
    "    SamMaskDecoderConfig,\n",
    "    SamModel,\n",
    "    SamProcessor,\n",
    "    SamImageProcessor\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047820e6-5b10-4818-b5af-052a8eeae8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_per_image(img):\n",
    "    \"\"\"\n",
    "    Normalize an image tensor by dividing each pixel value by the maximum value in the image (plus 1).\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor): Image tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized image tensor with values in the range [0, 1].\n",
    "    \"\"\"\n",
    "    # Compute the maximum value per image\n",
    "    max_val = img.amax(dim=(-1, -2), keepdim=True)  # Max value per channel\n",
    "    max_val = max_val + 1  # Add 1 to avoid division by zero\n",
    "\n",
    "    # Normalize by max value\n",
    "    normalized_img = img / max_val\n",
    "    return normalized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67235a91-733e-42a5-8d86-212378ef1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralExpandedDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the expanded dataset.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        \"\"\"\n",
    "        Scans the directory structure to find all saved samples.\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing file paths for each sample.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for sample_name in os.listdir(self.root_dir):\n",
    "            sample_path = os.path.join(self.root_dir, sample_name)\n",
    "            if not os.path.isdir(sample_path):\n",
    "                continue\n",
    "\n",
    "            # Collect file paths for bands, binary mask, and prompt\n",
    "            bands_path = os.path.join(sample_path, \"bands.pt\")\n",
    "            mask_path = os.path.join(sample_path, \"binary_mask.tif\")\n",
    "            prompt_path = os.path.join(sample_path, \"prompt.json\")\n",
    "\n",
    "            if os.path.exists(bands_path) and os.path.exists(mask_path) and os.path.exists(prompt_path):\n",
    "                samples.append({\n",
    "                    \"bands\": bands_path,\n",
    "                    \"mask\": mask_path,\n",
    "                    \"prompt\": prompt_path\n",
    "                })\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads a sample.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (prompt, bands, binary_mask)\n",
    "        \"\"\"\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        bands = None\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.load.*\")\n",
    "            bands = torch.load(sample[\"bands\"])\n",
    "\n",
    "        binary_mask = to_tensor(Image.open(sample[\"mask\"])).squeeze(0)  # Remove channel dimension\n",
    "\n",
    "        with open(sample[\"prompt\"], \"r\") as f:\n",
    "            prompt = json.load(f)\n",
    "\n",
    "        return prompt, normalize_per_image(bands), binary_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eed851a-2e3d-4c43-9498-2dfb49f59d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./expanded_dataset_output\"\n",
    "dataset = HyperspectralExpandedDataset(root_dir=root_dir)\n",
    "\n",
    "# Split dataset: 90% training, 10% evaluation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d67f67e-1f7e-46e3-a81e-0fc42d01bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10505\n",
      "{'centroid': [117.45, 2.15], 'random_point': [117, 1]}\n",
      "torch.Size([12, 120, 120])\n",
      "torch.Size([120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "for (prompt, img, mask) in train_dataset:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73117c52-9e00-47bd-820f-f4c88fc7c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'centroid': [tensor([ 73.6996,  20.8202,  72.0199,  24.2275, 118.0667,  76.6011,  41.8462,\n",
      "         97.5854, 114.5000,  18.2540,  64.5663, 101.1000,  66.0522,  38.6026,\n",
      "         51.6794,  97.4505, 117.8684,  39.6109,   6.2841, 113.7049,   3.7132,\n",
      "         54.9539,  37.4452,  14.4762,  72.3264, 112.5969, 109.1109,  81.0731,\n",
      "         94.3195,  86.0193, 105.7262,  84.5280,  59.5000,  63.3437,  15.8971,\n",
      "         11.1844,   8.3231,   8.6261,  93.1406,  74.0059,  19.2321,  59.9540,\n",
      "        111.2772, 116.9841,  38.7669,   2.4626,  91.5000,  20.2246,  15.6869,\n",
      "          0.2857,   2.8017,   2.5532,  61.0629, 117.6667,   3.7077, 112.5752,\n",
      "         32.0000,  14.4755,  72.5252,  99.6927, 112.7690,  30.4091,   5.6750,\n",
      "        114.3601,  75.6715,  47.9559, 103.0000,  13.7155, 114.9437,  15.1524,\n",
      "        112.2319,  91.0000,  53.0374,   1.4857,  63.8888, 116.4151,   3.7200,\n",
      "         17.5373,  65.1592,  94.3737, 113.3171,   6.8251, 119.0000,  14.1250,\n",
      "        104.7735, 119.0000,  77.1954,  54.1170,  63.0616,   4.5833, 104.7371,\n",
      "         32.7880,   0.0000,  17.2628,  66.8599,   2.5000,  71.5703,  14.8144,\n",
      "        115.0323,  99.6189,  87.8197,  39.9317,  96.3555, 104.3957,  71.5913,\n",
      "        101.1642,  53.4084, 105.5259,   9.4448,  97.5945,   1.8000,  22.5523,\n",
      "         10.7060,   0.3571,  95.0534,  16.7384,  79.4878,  33.6767,  14.2425,\n",
      "        109.6097,  77.7635,  54.0172,  44.7650,  56.8222,  24.3091,  64.3530,\n",
      "        106.6082,  14.2649], dtype=torch.float64), tensor([ 34.2689,   5.5987,   8.3722,  23.5013, 116.8667,  13.5654,   0.6923,\n",
      "         85.4050,   5.0000,  18.9390,  68.9452, 118.8000,  47.7225,  10.2074,\n",
      "         14.3771,  49.2501, 105.6842, 105.1217, 116.6023, 105.7213,  25.3062,\n",
      "         79.4110,  35.3065,  41.0974,  57.3857,  82.5242, 102.5502,  51.6265,\n",
      "         49.7011,  75.3343,   2.9048,  90.3741,  59.5000,  68.8576,  95.5250,\n",
      "        112.4721, 117.7538, 111.5168, 108.4854,  38.9412,  58.6096,  30.7682,\n",
      "        103.8877,  32.8889,  65.1085,  81.3129, 119.0000, 109.3099,  74.5585,\n",
      "          3.1429,  10.2328,   3.5319,  17.1163,   3.1667,  87.1897,  68.2902,\n",
      "          0.4500,  90.9683,  24.6146,  97.7314,  75.6013, 111.9889,   2.4250,\n",
      "         77.6898,  51.9884,  64.2034, 119.0000,  84.0122,  40.7887,  13.3741,\n",
      "         70.7947, 119.0000,   1.7570,   7.7143,  91.2437,   7.7453, 118.1600,\n",
      "        115.1381, 100.4060,  29.7638,  61.1561,  51.3359,  90.0000,   0.3750,\n",
      "        114.9017,  84.0000,  83.9504,  63.1674,  42.2545, 110.7135,  17.5041,\n",
      "         57.8698,   0.0000, 100.3376,  53.9318,   0.0000,  91.5918,  12.8504,\n",
      "          2.5968,  93.8858,  84.5942,  38.0779,   9.6466,   2.9174,  34.6398,\n",
      "        118.0448,  63.8578,  34.4442,  53.0638,  85.1164,  10.0500, 106.7884,\n",
      "         74.4162,   5.3571,  78.0440,  92.6420,  79.4997,  70.5375, 106.6657,\n",
      "         46.8637,  99.0978,   5.7644, 108.5441,  74.2881,   4.8545,  91.2319,\n",
      "          2.8247,   6.0676], dtype=torch.float64)], 'random_point': [tensor([ 36,  35,  79,  57, 118,  50,  42, 103, 116,  21,  41,  99,  32,  32,\n",
      "         45,  94, 118,  21,   3, 115,   1,  69,  72,   3,  44, 114, 103,  47,\n",
      "         84,  70,  96,  48,  78,  51,   2,   9,   1,  11, 104,  46,  13,  20,\n",
      "        115, 119,  31,   1,  91,  13,   5,   0,   3,   4,  62, 116,   4, 116,\n",
      "         28,  25, 107, 104, 119,   1,  14, 115, 110,  80, 102,  10, 113,  12,\n",
      "        113,  89,  44,   3, 108, 115,   3,   2,  90,  73, 110,   2, 119,  12,\n",
      "        106, 119,  74,  12,  54,  10, 114,  22,   0,  29,  83,   5, 112,  12,\n",
      "        112,  80,  60,  54, 105, 107, 107, 104,  43, 103,   7,  91,   2,   0,\n",
      "          3,   0,  69,  17,  75,   1,  19, 119,  69,  53,  19,  43,  23,  12,\n",
      "        102,   4]), tensor([ 39,   6,  11,   0, 119,   7,   1,  50,   4,   1,  98, 118,  47,  18,\n",
      "         17,  62, 111, 111, 116, 105,  26, 108,   8,  22,  42,  73,  96,  49,\n",
      "         68,  87,   3,  81,  85,  35, 118, 116, 118, 113, 112,  18,  48,  19,\n",
      "        113,  33, 107,  86, 119, 100,  74,   3,  19,   2,  14,   3,  81, 109,\n",
      "          0, 119,  51, 118,  85, 116,   0,  60, 108,  55, 119,  77,  42,  15,\n",
      "         57, 119,   0,   9, 114,   8, 119, 117, 110,  15,  61,  46,  90,   0,\n",
      "        116,  84, 119,  84,  65, 115,  16, 118,   0, 111,  86,   0, 108,  21,\n",
      "          0, 106,  91,   0,   2,   6,  39, 117,  82,  27,  46,  75,  10, 117,\n",
      "         74,   6,  55, 107, 105,  46, 108,  32, 102,  13, 108,  61,   1, 107,\n",
      "          5,   3])]}\n",
      "torch.Size([128, 12, 120, 120])\n",
      "torch.Size([128, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "for (prompt, img, mask) in train_loader:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071ec22a-1924-400a-838e-4f701230451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralSAM(nn.Module):\n",
    "    def __init__(self, sam_checkpoint=\"facebook/sam-vit-base\", num_input_channels=12):\n",
    "        \"\"\"\n",
    "        Adapt SAM for hyperspectral data by modifying the input layer to handle more channels\n",
    "        and adding a final layer for binary segmentation.\n",
    "\n",
    "        Args:\n",
    "            sam_checkpoint (str): Hugging Face SAM model checkpoint.\n",
    "            num_input_channels (int): Number of input channels for hyperspectral data.\n",
    "        \"\"\"\n",
    "        super(HyperspectralSAM, self).__init__()\n",
    "\n",
    "        vision_config = SamVisionConfig(num_channels=12, image_size=120)\n",
    "        decoder_config = SamMaskDecoderConfig(num_multimask_outputs = 1)\n",
    "        prompt_config  = SamPromptEncoderConfig(image_size=120)\n",
    "        \n",
    "        config = SamConfig(vision_config = vision_config, \n",
    "                           prompt_encoder_config = prompt_config, \n",
    "                           mask_decoder_config = decoder_config, \n",
    "                           name_or_path=sam_checkpoint\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # self.processor = SamProcessor(img_processor)\n",
    "        self.sam_model = SamModel.from_pretrained(sam_checkpoint, config=config, ignore_mismatched_sizes=True)\n",
    "        self.sam_model.train()\n",
    "    def forward(self, pixel_values, input_points=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the adapted SAM model.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (torch.Tensor): Input tensor of shape (batch_size, num_channels, height, width).\n",
    "            input_points (torch.Tensor, optional): Points as input prompts, of shape (batch_size, num_points, 2).\n",
    "            input_boxes (torch.Tensor, optional): Boxes as input prompts, of shape (batch_size, num_boxes, 4).\n",
    "            input_masks (torch.Tensor, optional): Masks as input prompts, of shape (batch_size, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary segmentation logits of shape (batch_size, 1, height, width).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.sam_model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_points=input_points\n",
    "        )\n",
    "        return outputs\n",
    "        # outputs[\"iou_scores\"]\n",
    "        # logits = self.final_conv(outputs[\"pred_masks\"][:, 0, :, :, :])\n",
    "        # return {\"pred_masks\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3f8ab7-f5ce-4df3-858d-3e3453ceadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(train_loader, model, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop for a model that processes image, mask, and prompt data from a train loader.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        model (torch.nn.Module): Model to train.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to use for training ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Resize and Normalize transformations\n",
    "    # Explicit normalization for RGB and hyperspectral bands\n",
    "    # normalize_img = Normalize(\n",
    "    #     mean=[0.5, 0.485, 0.456, 0.406] + [0.5] * 8,  # RGB + Hyperspectral\n",
    "    #     std=[0.5, 0.229, 0.224, 0.225] + [0.5] * 8   # RGB + Hyperspectral\n",
    "    # )\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, (prompt, img, mask) in progress_bar:\n",
    "        # Preprocess the image\n",
    "        # img = normalize_img(img)  # Normalize input images\n",
    "        img = img.to(device)  # Move to the correct device\n",
    "        # print(torch.min(img), torch.max(img))\n",
    "        # Preprocess the mask\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Preprocess input points\n",
    "        random_point_x, random_point_y = prompt['random_point']\n",
    "        random_point = torch.stack((random_point_x, random_point_y), dim=-1).to(device)  # Combine and move to device\n",
    "        random_point = random_point.unsqueeze(1).unsqueeze(2).to(device)  # Shape: (batch_size, 1, 1, 2)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(pixel_values=img, input_points=random_point)\n",
    "\n",
    "        # Resize mask to match the predictions' spatial dimensions\n",
    "        predictions_shape = predictions[\"pred_masks\"].shape[-2:]  # (height, width)\n",
    "        resize_mask = Resize(predictions_shape, antialias=True)  # Dynamically adjust mask size\n",
    "        mask = resize_mask(mask)\n",
    "        mask = (mask > 0.5).float()\n",
    "        # print(mask)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions[\"pred_masks\"], mask.unsqueeze(1).unsqueeze(1).float())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss over all batches\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training completed. Average Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ccd388-fb20-4b91-ae10-55be0f5736ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SamModel were not initialized from the model checkpoint at facebook/sam-vit-base and are newly initialized because the shapes did not match:\n",
      "- mask_decoder.iou_prediction_head.proj_out.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- mask_decoder.iou_prediction_head.proj_out.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- mask_decoder.mask_tokens.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.patch_embed.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 12, 16, 16]) in the model instantiated\n",
      "- vision_encoder.pos_embed: found shape torch.Size([1, 64, 64, 768]) in the checkpoint and torch.Size([1, 7, 7, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HyperspectralSAM(num_input_channels=12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d4f66e9-0134-4135-b15f-7c287b4273fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.7):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Apply sigmoid to logits with clamping for stability\n",
    "        probs = torch.sigmoid(logits).clamp(min=1e-6, max=1-1e-6)\n",
    "\n",
    "        # Flatten tensors for Dice and BCE calculations\n",
    "        probs_flat = probs.view(probs.size(0), -1)\n",
    "        targets_flat = targets.view(targets.size(0), -1)\n",
    "\n",
    "        # Weighted BCE loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        weight = torch.ones_like(targets) * 0.38  # Background weight\n",
    "        weight[targets == 1] = 1.0  # Foreground weight\n",
    "        bce_loss = (bce_loss * weight).mean()\n",
    "\n",
    "        # Dice loss with epsilon to avoid division by zero\n",
    "        intersection = (probs_flat * targets_flat).sum(dim=1)\n",
    "        dice_loss = 1 - (2.0 * intersection / (probs_flat.sum(dim=1) + targets_flat.sum(dim=1) + 1e-6)).mean()\n",
    "\n",
    "        # Combine BCE and Dice loss\n",
    "        total_loss = self.bce_weight * bce_loss + (1 - self.bce_weight) * dice_loss\n",
    "\n",
    "        # Validate loss for NaN\n",
    "        if not torch.isfinite(total_loss):\n",
    "            print(\"NaN detected in loss computation\")\n",
    "            print(f\"Logits: {logits}\")\n",
    "            print(f\"Targets: {targets}\")\n",
    "            print(f\"Probs: {probs}\")\n",
    "            print(f\"BCE Loss: {bce_loss}, Dice Loss: {dice_loss}\")\n",
    "            raise ValueError(\"Loss computation resulted in NaN\")\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f5908d-7a99-4ec4-9e61-df012d8f5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.4748010772305566\n",
      "Epoch 2/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.43] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.4410170288504781\n",
      "Epoch 3/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:17<00:00,  1.05s/batch, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.4366259160074028\n",
      "Epoch 4/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:28<00:00,  1.19s/batch, loss=0.432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.43109300571518977\n",
      "Epoch 5/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:25<00:00,  1.16s/batch, loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.41959352026114594\n",
      "Epoch 6/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:20<00:00,  1.09s/batch, loss=0.432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.4088892840050362\n",
      "Epoch 7/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.401325719582068\n",
      "Epoch 8/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3950336893668046\n",
      "Epoch 9/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:32<00:00,  1.25s/batch, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3884747020296148\n",
      "Epoch 10/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.23s/batch, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.38807787363593643\n",
      "Epoch 11/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.21s/batch, loss=0.381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.38283543006793874\n",
      "Epoch 12/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.23s/batch, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.38121634319021896\n",
      "Epoch 13/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:32<00:00,  1.26s/batch, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.38049358048954524\n",
      "Epoch 14/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:28<00:00,  1.19s/batch, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.37940340630106023\n",
      "Epoch 15/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.28s/batch, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3792246406948244\n",
      "Epoch 16/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:26<00:00,  1.17s/batch, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3793349068712544\n",
      "Epoch 17/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:27<00:00,  1.18s/batch, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.37890425203619776\n",
      "Epoch 18/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.23s/batch, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.37854889840693084\n",
      "Epoch 19/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.24s/batch, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.37823318065823736\n",
      "Epoch 20/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.23s/batch, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.37779253600416957\n",
      "Epoch 21/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.24s/batch, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3753152175529583\n",
      "Epoch 22/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.24s/batch, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3749855849388483\n",
      "Epoch 23/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.37398003283384684\n",
      "Epoch 24/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:36<00:00,  1.30s/batch, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3746216240766886\n",
      "Epoch 25/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.21s/batch, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3738975488656276\n",
      "Epoch 26/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.21s/batch, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3734238937094405\n",
      "Epoch 27/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.23s/batch, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.37358608680802424\n",
      "Epoch 28/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3730669992195593\n",
      "Epoch 29/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:35<00:00,  1.29s/batch, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.373314478912869\n",
      "Epoch 30/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:32<00:00,  1.24s/batch, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3727270517800305\n",
      "Epoch 31/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:58<00:00,  1.60s/batch, loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.37044317577336283\n",
      "Epoch 32/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:41<00:00,  1.37s/batch, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.36900631036307363\n",
      "Epoch 33/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.28s/batch, loss=0.358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.36906491058903773\n",
      "Epoch 34/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:28<00:00,  1.19s/batch, loss=0.381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.36839548719895854\n",
      "Epoch 35/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:35<00:00,  1.29s/batch, loss=0.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.367613024405531\n",
      "Epoch 36/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.27s/batch, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3679727420613572\n",
      "Epoch 37/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:28<00:00,  1.20s/batch, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3669643273224702\n",
      "Epoch 38/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.24s/batch, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3661971325809891\n",
      "Epoch 39/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:33<00:00,  1.26s/batch, loss=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.36744203599723607\n",
      "Epoch 40/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.23s/batch, loss=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3662784703679987\n",
      "Epoch 41/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.28s/batch, loss=0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.36385917864941264\n",
      "Epoch 42/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:32<00:00,  1.25s/batch, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.36279071786919154\n",
      "Epoch 43/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:36<00:00,  1.30s/batch, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3621385987545993\n",
      "Epoch 44/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:32<00:00,  1.25s/batch, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.36204983817564473\n",
      "Epoch 45/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.36134609700860204\n",
      "Epoch 46/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3607161463917913\n",
      "Epoch 47/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:35<00:00,  1.29s/batch, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3604062831885106\n",
      "Epoch 48/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.27s/batch, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.3605216342855144\n",
      "Epoch 49/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.27s/batch, loss=0.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.35968271501966426\n",
      "Epoch 50/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:39<00:00,  1.34s/batch, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.35973068266301544\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "experiment_name = \"random_prompt\"\n",
    "epochs = 50\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = DiceBCELoss()  # For binary segmentation masks\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Scheduler to halve the learning rate every 5 epochs\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Current LR: {scheduler.get_last_lr()}\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(train_loader, model, optimizer, criterion, device)\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    save_dir = f\"models/{experiment_name}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = f\"{save_dir}/hyperspectral_sam_epoch_{epoch+1}.pt\"\n",
    "    torch.save(model, model_path)\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35047639-5b36-4dea-b216-67c83aca2e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19850e4-c83f-43d9-92fe-84b5ed4f3f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7116d-6bd0-4f80-b008-203426a70933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
