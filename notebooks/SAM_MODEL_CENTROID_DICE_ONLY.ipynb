{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89920070-c7c6-4550-bd95-e01a299e6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    SamVisionConfig,\n",
    "    SamPromptEncoderConfig,\n",
    "    SamMaskDecoderConfig,\n",
    "    SamModel,\n",
    "    SamProcessor,\n",
    "    SamImageProcessor\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047820e6-5b10-4818-b5af-052a8eeae8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_per_image(img):\n",
    "    \"\"\"\n",
    "    Normalize an image tensor by dividing each pixel value by the maximum value in the image (plus 1).\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor): Image tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized image tensor with values in the range [0, 1].\n",
    "    \"\"\"\n",
    "    # Compute the maximum value per image\n",
    "    max_val = img.amax(dim=(-1, -2), keepdim=True)  # Max value per channel\n",
    "    max_val = max_val + 1  # Add 1 to avoid division by zero\n",
    "\n",
    "    # Normalize by max value\n",
    "    normalized_img = img / max_val\n",
    "    return normalized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67235a91-733e-42a5-8d86-212378ef1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralExpandedDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the expanded dataset.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        \"\"\"\n",
    "        Scans the directory structure to find all saved samples.\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing file paths for each sample.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for sample_name in os.listdir(self.root_dir):\n",
    "            sample_path = os.path.join(self.root_dir, sample_name)\n",
    "            if not os.path.isdir(sample_path):\n",
    "                continue\n",
    "\n",
    "            # Collect file paths for bands, binary mask, and prompt\n",
    "            bands_path = os.path.join(sample_path, \"bands.pt\")\n",
    "            mask_path = os.path.join(sample_path, \"binary_mask.tif\")\n",
    "            prompt_path = os.path.join(sample_path, \"prompt.json\")\n",
    "\n",
    "            if os.path.exists(bands_path) and os.path.exists(mask_path) and os.path.exists(prompt_path):\n",
    "                samples.append({\n",
    "                    \"bands\": bands_path,\n",
    "                    \"mask\": mask_path,\n",
    "                    \"prompt\": prompt_path\n",
    "                })\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads a sample.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (prompt, bands, binary_mask)\n",
    "        \"\"\"\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        bands = None\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.load.*\")\n",
    "            bands = torch.load(sample[\"bands\"])\n",
    "\n",
    "        binary_mask = to_tensor(Image.open(sample[\"mask\"])).squeeze(0)  # Remove channel dimension\n",
    "\n",
    "        with open(sample[\"prompt\"], \"r\") as f:\n",
    "            prompt = json.load(f)\n",
    "\n",
    "        return prompt, normalize_per_image(bands), binary_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eed851a-2e3d-4c43-9498-2dfb49f59d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./expanded_dataset_output\"\n",
    "dataset = HyperspectralExpandedDataset(root_dir=root_dir)\n",
    "\n",
    "# Split dataset: 90% training, 10% evaluation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d67f67e-1f7e-46e3-a81e-0fc42d01bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10505\n",
      "{'centroid': [28.445252158109952, 59.22353475692867], 'random_point': [61, 76]}\n",
      "torch.Size([12, 120, 120])\n",
      "torch.Size([120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "for (prompt, img, mask) in train_dataset:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73117c52-9e00-47bd-820f-f4c88fc7c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'centroid': [tensor([100.1844,  77.7186,  48.0732,  60.1692,  93.5194,  67.8435,  18.2421,\n",
      "         56.2103,  43.7935, 105.6491,  89.0308,  71.0735, 100.2617,   2.0000,\n",
      "         54.6781,  48.0000,  40.7679,  93.3576,  64.7331, 116.4773,  63.0625,\n",
      "         48.4096,  21.5341, 109.8931,  88.9138,  37.2393,   3.7972, 101.4298,\n",
      "         17.7072,  58.7567,  54.4813, 100.5000, 117.7260, 118.1600, 118.4118,\n",
      "         82.7295,  51.1835,  73.1219,  63.0000, 103.7015,  60.5992,  44.5043,\n",
      "        104.6299,   5.7179,   3.5556, 113.7377, 112.3469,   5.5250,  81.3037,\n",
      "        114.5000,  37.9047, 108.9514,  99.9271,   8.7778, 100.3565,  26.5843,\n",
      "         58.0617, 118.3750, 117.0000,  81.1758,   6.4023, 115.0000,   8.0877,\n",
      "         14.1176,  25.3725,   9.8534, 109.3719,  38.4598,  85.9409,  11.2918,\n",
      "        116.6875,  30.5050,  12.6373,   0.7895,  24.0988,  58.0644,  81.0260,\n",
      "        117.0000,  59.5000,  54.3667,  22.3966,  85.3821,  36.3410,  37.5990,\n",
      "         97.4845,   2.1176,  92.5618,  56.8887, 100.0341,  69.7404,  63.9149,\n",
      "         24.8556,  50.4817,   9.9564,  38.9199, 101.9630,   5.4483,  60.2420,\n",
      "         32.1800,  83.3733, 113.4851,  58.9868,  13.7298,   9.8512,  59.6843,\n",
      "         12.4643,  18.8047,  21.8217,  79.5484,  99.9006,  49.8333, 114.8065,\n",
      "         52.4874, 118.0588,   8.2000,  13.6960,  10.7059,  63.9353,  60.3792,\n",
      "        118.6667,  11.2941,   1.0192,   5.7802,  13.7981,  89.7882, 118.6000,\n",
      "         95.3519, 104.7371], dtype=torch.float64), tensor([ 42.4922, 106.6900,  42.1920,   9.0254,   9.6065,   2.7891,  13.0538,\n",
      "         84.9214,   4.0435,  11.1605,  98.9991,  89.5496, 115.7919,   0.0000,\n",
      "         62.8474, 119.0000, 107.5259,  48.5494,  59.1893,   6.0114,  67.9976,\n",
      "         58.1017,  15.6314,   9.8219,  21.6057,  20.8787,  61.5035,  82.6630,\n",
      "         35.6017,  72.2744,   2.4346,   0.0000,  76.7945, 113.3200,  50.1765,\n",
      "         93.8155,  92.8196,  53.1229, 118.0000, 101.5591,  24.1070,  53.8921,\n",
      "          4.8672, 108.7036,   0.6111,   1.9180,  88.8010,   2.5000, 114.1804,\n",
      "        117.5000,  19.7183, 106.3435,  46.5266,  70.0481,  63.3900,  83.6928,\n",
      "         84.8679,   1.1250, 118.7500,  13.5678, 112.4540,   0.0000,  22.9825,\n",
      "        118.5294,  98.2101,  90.6121,  74.0212, 115.5759,  59.9096, 113.4630,\n",
      "         65.8958, 114.8911,  61.3313, 107.0526,  70.3597, 117.0545,  84.4187,\n",
      "          0.2500,  59.5000,   1.1667,  22.4207,  32.9039,   7.4933,  82.1811,\n",
      "         25.2898, 118.0000,  82.0695,  75.7425,   3.7273,  46.2877,   4.2394,\n",
      "         95.4792,  79.4371, 104.7073,  81.8793, 100.2783,   4.1983,  59.2445,\n",
      "         40.0305,  89.2151,  92.5746,  66.2920,  14.3864, 106.7202,  25.7643,\n",
      "         34.3165,   3.8462,  41.3204,   6.4032,  27.0652, 118.1667, 115.9516,\n",
      "         39.8079,  32.2941, 118.4000, 102.3524, 112.0168,  20.2159,  48.0413,\n",
      "        111.5000,  82.4374, 108.6346,  80.8462,  77.4519,  13.5928,  85.2000,\n",
      "        103.4548,  17.5041], dtype=torch.float64)], 'random_point': [tensor([ 99,  65,  77,  40, 109,  66,  10,  57,  41, 114,  92,  36, 104,   0,\n",
      "         77,  48,  26, 117,  50, 119,  83,  58,   9, 101,  78,  38,   6,  99,\n",
      "         15,  99,  67, 102, 119, 119, 118,  68,  39,  45,  63, 104,  51,  23,\n",
      "        113,   6,   0, 114, 116,   8, 106, 116,  68, 107,  78,   6, 113,  21,\n",
      "         69, 119, 118, 117,   6, 115,   8,  15,  35,   5, 119,  32,  79,   3,\n",
      "        119,  30,   8,   0,  35,  79, 113, 117, 109,  55,  28, 116,  11,  40,\n",
      "         73,   0,  49,  95, 102,  56,  57,  14,   6,  13,  45, 108,   1,  25,\n",
      "          0,  91, 119,  75,  13,  12,  98,  19,  24,  31,  77,  84,  48, 117,\n",
      "         47, 118,  10,  29,   0, 116,  89, 119,   2,   1,   9,   3,  99, 118,\n",
      "        118, 114]), tensor([ 43, 115,  78,  10,  23,   1,  17,  51,   6,  15,  79,  97, 117,   0,\n",
      "          8, 119, 110,  63,  27,  11,  59,  49,  33,  10,  13,  34,  65,  56,\n",
      "         28,  48,   4,   0,  81, 109,  47, 112,  98,   0, 118, 110,  10,  44,\n",
      "          6, 115,   0,   2,  93,   2, 119, 118,  25, 104,  15,  77, 119,  88,\n",
      "         78,   2, 119,   1, 114,   0,  23, 118, 104,  94,  60, 111,  42, 117,\n",
      "         66, 114,  75, 104, 100, 118,  48,   0,  39,   0,  11,  47,   4,  88,\n",
      "          4, 118,  90,  98,   8,   9,   1,  62,  37, 107,  95,  83,   8,  35,\n",
      "          1,  93,  84,  62,  12, 109,  44,  31,   6,  27,   8,  26, 119, 118,\n",
      "         61,  31, 118,  96, 113,   0,  34, 112,  56, 110,  73,  82,   0,  86,\n",
      "         82,  16])]}\n",
      "torch.Size([128, 12, 120, 120])\n",
      "torch.Size([128, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "for (prompt, img, mask) in train_loader:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071ec22a-1924-400a-838e-4f701230451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralSAM(nn.Module):\n",
    "    def __init__(self, sam_checkpoint=\"facebook/sam-vit-base\", num_input_channels=12):\n",
    "        \"\"\"\n",
    "        Adapt SAM for hyperspectral data by modifying the input layer to handle more channels\n",
    "        and adding a final layer for binary segmentation.\n",
    "\n",
    "        Args:\n",
    "            sam_checkpoint (str): Hugging Face SAM model checkpoint.\n",
    "            num_input_channels (int): Number of input channels for hyperspectral data.\n",
    "        \"\"\"\n",
    "        super(HyperspectralSAM, self).__init__()\n",
    "\n",
    "        vision_config = SamVisionConfig(num_channels=12, image_size=120)\n",
    "        decoder_config = SamMaskDecoderConfig(num_multimask_outputs = 1)\n",
    "        prompt_config  = SamPromptEncoderConfig(image_size=120)\n",
    "        \n",
    "        config = SamConfig(vision_config = vision_config, \n",
    "                           prompt_encoder_config = prompt_config, \n",
    "                           mask_decoder_config = decoder_config, \n",
    "                           name_or_path=sam_checkpoint\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # self.processor = SamProcessor(img_processor)\n",
    "        self.sam_model = SamModel.from_pretrained(sam_checkpoint, config=config, ignore_mismatched_sizes=True)\n",
    "        self.sam_model.train()\n",
    "    def forward(self, pixel_values, input_points=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the adapted SAM model.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (torch.Tensor): Input tensor of shape (batch_size, num_channels, height, width).\n",
    "            input_points (torch.Tensor, optional): Points as input prompts, of shape (batch_size, num_points, 2).\n",
    "            input_boxes (torch.Tensor, optional): Boxes as input prompts, of shape (batch_size, num_boxes, 4).\n",
    "            input_masks (torch.Tensor, optional): Masks as input prompts, of shape (batch_size, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary segmentation logits of shape (batch_size, 1, height, width).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.sam_model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_points=input_points\n",
    "        )\n",
    "        return outputs\n",
    "        # outputs[\"iou_scores\"]\n",
    "        # logits = self.final_conv(outputs[\"pred_masks\"][:, 0, :, :, :])\n",
    "        # return {\"pred_masks\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3f8ab7-f5ce-4df3-858d-3e3453ceadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(train_loader, model, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop for a model that processes image, mask, and prompt data from a train loader.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        model (torch.nn.Module): Model to train.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to use for training ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Resize and Normalize transformations\n",
    "    # Explicit normalization for RGB and hyperspectral bands\n",
    "    # normalize_img = Normalize(\n",
    "    #     mean=[0.5, 0.485, 0.456, 0.406] + [0.5] * 8,  # RGB + Hyperspectral\n",
    "    #     std=[0.5, 0.229, 0.224, 0.225] + [0.5] * 8   # RGB + Hyperspectral\n",
    "    # )\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, (prompt, img, mask) in progress_bar:\n",
    "        # Preprocess the image\n",
    "        # img = normalize_img(img)  # Normalize input images\n",
    "        img = img.to(device)  # Move to the correct device\n",
    "        # print(torch.min(img), torch.max(img))\n",
    "        # Preprocess the mask\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Preprocess input points\n",
    "        random_point_x, random_point_y = prompt['centroid']\n",
    "        random_point = torch.stack((random_point_x, random_point_y), dim=-1).to(device)  # Combine and move to device\n",
    "        random_point = random_point.unsqueeze(1).unsqueeze(2).to(device)  # Shape: (batch_size, 1, 1, 2)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(pixel_values=img, input_points=random_point)\n",
    "\n",
    "        # Resize mask to match the predictions' spatial dimensions\n",
    "        predictions_shape = predictions[\"pred_masks\"].shape[-2:]  # (height, width)\n",
    "        resize_mask = Resize(predictions_shape, antialias=True)  # Dynamically adjust mask size\n",
    "        mask = resize_mask(mask)\n",
    "        mask = (mask > 0.5).float()\n",
    "        # print(mask)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions[\"pred_masks\"], mask.unsqueeze(1).unsqueeze(1).float())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss over all batches\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training completed. Average Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ccd388-fb20-4b91-ae10-55be0f5736ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SamModel were not initialized from the model checkpoint at facebook/sam-vit-base and are newly initialized because the shapes did not match:\n",
      "- mask_decoder.iou_prediction_head.proj_out.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- mask_decoder.iou_prediction_head.proj_out.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- mask_decoder.mask_tokens.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.patch_embed.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 12, 16, 16]) in the model instantiated\n",
      "- vision_encoder.pos_embed: found shape torch.Size([1, 64, 64, 768]) in the checkpoint and torch.Size([1, 7, 7, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HyperspectralSAM(num_input_channels=12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d4f66e9-0134-4135-b15f-7c287b4273fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.7):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Apply sigmoid to logits with clamping for stability\n",
    "        probs = torch.sigmoid(logits).clamp(min=1e-6, max=1-1e-6)\n",
    "\n",
    "        # Flatten tensors for Dice and BCE calculations\n",
    "        probs_flat = probs.view(probs.size(0), -1)\n",
    "        targets_flat = targets.view(targets.size(0), -1)\n",
    "\n",
    "        # Dice loss with epsilon to avoid division by zero\n",
    "        intersection = (probs_flat * targets_flat).sum(dim=1)\n",
    "        dice_loss = 1 - (2.0 * intersection / (probs_flat.sum(dim=1) + targets_flat.sum(dim=1) + 1e-6)).mean()\n",
    "\n",
    "        # Combine BCE and Dice loss\n",
    "        total_loss = dice_loss\n",
    "\n",
    "        # Validate loss for NaN\n",
    "        if not torch.isfinite(total_loss):\n",
    "            print(\"NaN detected in loss computation\")\n",
    "            print(f\"Logits: {logits}\")\n",
    "            print(f\"Targets: {targets}\")\n",
    "            print(f\"Probs: {probs}\")\n",
    "            print(f\"BCE Loss: {bce_loss}, Dice Loss: {dice_loss}\")\n",
    "            raise ValueError(\"Loss computation resulted in NaN\")\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f5908d-7a99-4ec4-9e61-df012d8f5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [03:08<00:00,  2.54s/batch, loss=0.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7863446071341231\n",
      "Epoch 2/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.02s/batch, loss=0.776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834294881369617\n",
      "Epoch 3/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.01s/batch, loss=0.764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834054336354539\n",
      "Epoch 4/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:17<00:00,  1.04s/batch, loss=0.802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834778476405788\n",
      "Epoch 5/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.01s/batch, loss=0.804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834811299233824\n",
      "Epoch 6/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.03s/batch, loss=0.805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834840851861078\n",
      "Epoch 7/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.01s/batch, loss=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834325811347446\n",
      "Epoch 8/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.01s/batch, loss=0.782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834406599805162\n",
      "Epoch 9/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.00batch/s, loss=0.78] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834365456490904\n",
      "Epoch 10/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.02s/batch, loss=0.786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834481210321993\n",
      "Epoch 11/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.03s/batch, loss=0.8]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834748835177034\n",
      "Epoch 12/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834889365209116\n",
      "Epoch 13/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.02s/batch, loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834010277245496\n",
      "Epoch 14/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:17<00:00,  1.04s/batch, loss=0.779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834340543360323\n",
      "Epoch 15/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.03s/batch, loss=0.783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834415105549065\n",
      "Epoch 16/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.02s/batch, loss=0.753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7833840444281295\n",
      "Epoch 17/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834969686495291\n",
      "Epoch 18/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834694361364519\n",
      "Epoch 19/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.00s/batch, loss=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7833422704322918\n",
      "Epoch 20/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:17<00:00,  1.04s/batch, loss=0.786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834468443651457\n",
      "Epoch 21/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.00s/batch, loss=0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.783445623275396\n",
      "Epoch 22/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.01batch/s, loss=0.798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834711453399142\n",
      "Epoch 23/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:12<00:00,  1.01batch/s, loss=0.766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834089180907687\n",
      "Epoch 24/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.00batch/s, loss=0.811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.783494966255652\n",
      "Epoch 25/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.01s/batch, loss=0.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834383007642385\n",
      "Epoch 26/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.02s/batch, loss=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834565099832174\n",
      "Epoch 27/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7833684247893256\n",
      "Epoch 28/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.01batch/s, loss=0.746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7833720163719075\n",
      "Epoch 29/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.01batch/s, loss=0.797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834694594950289\n",
      "Epoch 30/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.00batch/s, loss=0.794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834624905843992\n",
      "Epoch 31/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.01s/batch, loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7833459143703049\n",
      "Epoch 32/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:15<00:00,  1.02s/batch, loss=0.789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.783453102047379\n",
      "Epoch 33/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.00batch/s, loss=0.786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834468040917371\n",
      "Epoch 34/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.01batch/s, loss=0.775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834266923569344\n",
      "Epoch 35/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.00batch/s, loss=0.848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7835655719847292\n",
      "Epoch 36/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.00s/batch, loss=0.773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834237999207264\n",
      "Epoch 37/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.01batch/s, loss=0.729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7833387505363774\n",
      "Epoch 38/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.00batch/s, loss=0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834123783820385\n",
      "Epoch 39/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.01s/batch, loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7833580439155167\n",
      "Epoch 40/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.00s/batch, loss=0.803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834808608970126\n",
      "Epoch 41/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.01batch/s, loss=0.83] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7835309263822194\n",
      "Epoch 42/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:13<00:00,  1.00batch/s, loss=0.786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834476256692732\n",
      "Epoch 43/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.01s/batch, loss=0.793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834617551919576\n",
      "Epoch 44/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:17<00:00,  1.05s/batch, loss=0.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.783437393001608\n",
      "Epoch 45/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:19<00:00,  1.08s/batch, loss=0.768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834137307631003\n",
      "Epoch 46/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:18<00:00,  1.06s/batch, loss=0.762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834012460064244\n",
      "Epoch 47/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:17<00:00,  1.04s/batch, loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834003616023708\n",
      "Epoch 48/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7834614918038652\n",
      "Epoch 49/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.7833653559555879\n",
      "Epoch 50/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:14<00:00,  1.01s/batch, loss=0.796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.783465759979712\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "experiment_name = \"centroid_prompt_dice_only\"\n",
    "epochs = 50\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = DiceBCELoss()  # For binary segmentation masks\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Scheduler to halve the learning rate every 5 epochs\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Current LR: {scheduler.get_last_lr()}\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(train_loader, model, optimizer, criterion, device)\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    save_dir = f\"models/{experiment_name}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = f\"{save_dir}/hyperspectral_sam_epoch_{epoch+1}.pt\"\n",
    "    torch.save(model, model_path)\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35047639-5b36-4dea-b216-67c83aca2e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19850e4-c83f-43d9-92fe-84b5ed4f3f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7116d-6bd0-4f80-b008-203426a70933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
