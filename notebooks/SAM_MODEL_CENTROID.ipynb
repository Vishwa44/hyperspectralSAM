{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89920070-c7c6-4550-bd95-e01a299e6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    SamVisionConfig,\n",
    "    SamPromptEncoderConfig,\n",
    "    SamMaskDecoderConfig,\n",
    "    SamModel,\n",
    "    SamProcessor,\n",
    "    SamImageProcessor\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047820e6-5b10-4818-b5af-052a8eeae8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_per_image(img):\n",
    "    \"\"\"\n",
    "    Normalize an image tensor by dividing each pixel value by the maximum value in the image (plus 1).\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor): Image tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized image tensor with values in the range [0, 1].\n",
    "    \"\"\"\n",
    "    # Compute the maximum value per image\n",
    "    max_val = img.amax(dim=(-1, -2), keepdim=True)  # Max value per channel\n",
    "    max_val = max_val + 1  # Add 1 to avoid division by zero\n",
    "\n",
    "    # Normalize by max value\n",
    "    normalized_img = img / max_val\n",
    "    return normalized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67235a91-733e-42a5-8d86-212378ef1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralExpandedDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the expanded dataset.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        \"\"\"\n",
    "        Scans the directory structure to find all saved samples.\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing file paths for each sample.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for sample_name in os.listdir(self.root_dir):\n",
    "            sample_path = os.path.join(self.root_dir, sample_name)\n",
    "            if not os.path.isdir(sample_path):\n",
    "                continue\n",
    "\n",
    "            # Collect file paths for bands, binary mask, and prompt\n",
    "            bands_path = os.path.join(sample_path, \"bands.pt\")\n",
    "            mask_path = os.path.join(sample_path, \"binary_mask.tif\")\n",
    "            prompt_path = os.path.join(sample_path, \"prompt.json\")\n",
    "\n",
    "            if os.path.exists(bands_path) and os.path.exists(mask_path) and os.path.exists(prompt_path):\n",
    "                samples.append({\n",
    "                    \"bands\": bands_path,\n",
    "                    \"mask\": mask_path,\n",
    "                    \"prompt\": prompt_path\n",
    "                })\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads a sample.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (prompt, bands, binary_mask)\n",
    "        \"\"\"\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        bands = None\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.load.*\")\n",
    "            bands = torch.load(sample[\"bands\"])\n",
    "\n",
    "        binary_mask = to_tensor(Image.open(sample[\"mask\"])).squeeze(0)  # Remove channel dimension\n",
    "\n",
    "        with open(sample[\"prompt\"], \"r\") as f:\n",
    "            prompt = json.load(f)\n",
    "\n",
    "        return prompt, normalize_per_image(bands), binary_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eed851a-2e3d-4c43-9498-2dfb49f59d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./expanded_dataset_output\"\n",
    "dataset = HyperspectralExpandedDataset(root_dir=root_dir)\n",
    "\n",
    "# Split dataset: 90% training, 10% evaluation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d67f67e-1f7e-46e3-a81e-0fc42d01bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10505\n",
      "{'centroid': [22.37883169462117, 17.358010410641988], 'random_point': [26, 1]}\n",
      "torch.Size([12, 120, 120])\n",
      "torch.Size([120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "for (prompt, img, mask) in train_dataset:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73117c52-9e00-47bd-820f-f4c88fc7c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'centroid': [tensor([105.7406,  59.5000,  93.1959,   9.7339,  87.6094,  15.9882,  74.9401,\n",
      "         94.2621, 111.3608, 109.7624, 108.0814,   1.4706,  39.8946,  85.4658,\n",
      "          5.8182,  31.7907,  60.0295,  85.5452,  30.9752,  18.6792,   8.1914,\n",
      "          1.3696,  30.4769, 117.7143,  58.6408,  61.7861,   8.1685,  24.5406,\n",
      "         66.8777,  59.1497,  58.8262, 114.2101,   4.1600,  93.2593,  17.6250,\n",
      "         52.3945,  70.6667,  17.6201,   0.3636,   8.0444,  12.7674,  87.1157,\n",
      "          7.6866, 109.4115,  50.8318, 109.2927,   7.3060,  42.4276,  17.9211,\n",
      "         60.3002, 111.3866, 109.4826,  85.4127,  17.6509,  47.6922, 104.7000,\n",
      "         24.4521, 103.7389,  55.2075, 109.7931,   5.8421, 111.9107,  10.4127,\n",
      "         61.3568,  59.6406,   8.4538,  89.6814, 117.7260,  59.9958,   5.7348,\n",
      "         48.0000, 113.8051, 113.4851,  45.9837,   5.4828,   9.2998,  71.6631,\n",
      "          2.0000,  42.1155, 108.7899, 117.4000,  75.6263, 116.7391,  65.1853,\n",
      "        111.6468,  24.0650,  33.7565,  34.6345,  94.8183,  51.1460,  18.7521,\n",
      "          8.0000,  97.0860, 119.0000,  69.1372,   2.2838, 101.0000, 104.0649,\n",
      "         99.0400,  53.6949,  46.7116,  10.5000,  58.0775,  46.1636,  50.4738,\n",
      "         53.0374,  56.5828,  81.4677,  38.5478,  84.4818, 110.8008,  61.4250,\n",
      "        107.1257, 105.1437,  14.5316,  17.1344,  31.7177,   7.8676, 116.8739,\n",
      "         18.8446,  46.2034, 118.4286, 117.1923,  78.5299,  61.3774,  18.6242,\n",
      "          8.5607,  32.5039], dtype=torch.float64), tensor([104.7782,  59.5000,  59.3027, 116.9817,  56.6100, 111.3575,  37.3331,\n",
      "         52.2678,  25.6430,   1.9901,  68.1253, 117.5294,  26.8871,  43.6194,\n",
      "          4.5385,  38.2027,  75.1153,  81.2140,  90.7400,  73.8783,   8.1523,\n",
      "        100.4565,  32.8709,  75.1429,  59.6558,  73.2681,   5.3533,  32.1406,\n",
      "        112.8712,  47.9904,  63.7742, 114.4454,  91.3867,  61.1230, 118.6250,\n",
      "         49.2507,  31.7557,  42.8460,  28.5909,  43.0370,  97.5321,   8.3067,\n",
      "         61.0085,  97.2174,  83.7652,   3.2195,  24.0996, 100.0233, 104.7940,\n",
      "        112.6476,  30.2411,   5.3980,  94.7389,  70.3865,  75.8514, 118.7000,\n",
      "        117.7945, 106.4801,  52.8079,   0.9483,  52.2171,  18.6331,  15.8713,\n",
      "          5.7060,  51.5839,  26.4425,  73.2654,  76.7945,  59.9469,  57.4116,\n",
      "         90.0000, 110.0975,  92.5746,  98.3536, 118.1379, 101.5655,  56.2108,\n",
      "         72.2619,  70.7747,  33.1569,   2.7000,  86.2136,  52.3261, 116.3147,\n",
      "        100.2388,  25.0179,   2.5130,  18.1848,  90.5002,  22.2251,  10.5967,\n",
      "          0.0000,  64.8729,   0.5000,  33.9364,  69.0676,   0.0000,  25.3493,\n",
      "        118.3600,  51.5006,  51.5958, 119.0000,  35.2514,  50.6561,   7.2544,\n",
      "          1.7570,  38.3426,  57.2744,  92.9763, 106.3636,  14.6275,  25.3419,\n",
      "         19.6757,  98.2801,  84.8007, 100.4859,  28.6809,  46.2402, 108.6555,\n",
      "        115.0469, 116.4237,  65.4286, 110.4744,  45.9611,  43.8120,  23.7975,\n",
      "        111.8250,   5.5079], dtype=torch.float64)], 'random_point': [tensor([ 92,  86, 107,  16, 116,   9,  80, 102,  92, 115, 111,   2,  17,  89,\n",
      "          5,  10,  31,  94,   8,  18,  13,   2,   1, 117,  48, 103,   3,  39,\n",
      "         70,  74,  19, 112,   0,  81,  18,  49,  10,  10,   0,  16,  22,  70,\n",
      "         11, 110,   7, 111,   5,  17,  14,  80, 101, 101,  73,   8,  64, 106,\n",
      "         32, 101,  29, 109,   8, 118,  10,  73,  17,   5,  66, 119,   8,   2,\n",
      "         48, 115, 119,  69,   8,   9,  89,   2,  10, 107, 117,  60, 119,  70,\n",
      "        115,  17,  39,  10, 100,  32,  23,   9,  96, 119,  89,   3, 101,  88,\n",
      "        100,  37,  56,  10, 115,  10,  55,  44,  84, 116,  42,  79, 112,  30,\n",
      "        119, 109,  25,   4,  27,   6, 119,  14,  41, 119, 118,  60,   1,  14,\n",
      "         13,  44]), tensor([ 93,  66,  67, 119,  34, 102,  40,  53,   8,   4,  65, 118,  40,  42,\n",
      "          6,  44, 109,  92,  77,  96,  15,  98,  60,  76,  22,  42,   6,  33,\n",
      "        118, 100,  46, 117,  87,  51, 118,  89,  26,  59,  29,  46,  89,   2,\n",
      "         54,  79, 102,   9,  15,  78, 114, 108,  32,   1,  89, 117,  27, 119,\n",
      "        116, 115,  69,   2,  49,  41,  16,   7,  55,  28,  99,  81,  45,  67,\n",
      "         90, 111,  84, 104, 118, 107,  86,  75,  38,  22,   2,  98,  54, 117,\n",
      "         97,  28,   1,   8,  74,  28,   7,   0,  52,   0,  42,  68,   0,  32,\n",
      "        119,  18, 101, 119,  29,  56,  10,   0,  44,  43,  97, 104,  10,   0,\n",
      "         19, 118,  86,  78,  47,  48, 109, 111, 115,  67, 105,  53,  30,  26,\n",
      "        106,   2])]}\n",
      "torch.Size([128, 12, 120, 120])\n",
      "torch.Size([128, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "for (prompt, img, mask) in train_loader:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071ec22a-1924-400a-838e-4f701230451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralSAM(nn.Module):\n",
    "    def __init__(self, sam_checkpoint=\"facebook/sam-vit-base\", num_input_channels=12):\n",
    "        \"\"\"\n",
    "        Adapt SAM for hyperspectral data by modifying the input layer to handle more channels\n",
    "        and adding a final layer for binary segmentation.\n",
    "\n",
    "        Args:\n",
    "            sam_checkpoint (str): Hugging Face SAM model checkpoint.\n",
    "            num_input_channels (int): Number of input channels for hyperspectral data.\n",
    "        \"\"\"\n",
    "        super(HyperspectralSAM, self).__init__()\n",
    "\n",
    "        vision_config = SamVisionConfig(num_channels=12, image_size=120)\n",
    "        decoder_config = SamMaskDecoderConfig(num_multimask_outputs = 1)\n",
    "        prompt_config  = SamPromptEncoderConfig(image_size=120)\n",
    "        \n",
    "        config = SamConfig(vision_config = vision_config, \n",
    "                           prompt_encoder_config = prompt_config, \n",
    "                           mask_decoder_config = decoder_config, \n",
    "                           name_or_path=sam_checkpoint\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # self.processor = SamProcessor(img_processor)\n",
    "        self.sam_model = SamModel.from_pretrained(sam_checkpoint, config=config, ignore_mismatched_sizes=True)\n",
    "        self.sam_model.train()\n",
    "    def forward(self, pixel_values, input_points=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the adapted SAM model.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (torch.Tensor): Input tensor of shape (batch_size, num_channels, height, width).\n",
    "            input_points (torch.Tensor, optional): Points as input prompts, of shape (batch_size, num_points, 2).\n",
    "            input_boxes (torch.Tensor, optional): Boxes as input prompts, of shape (batch_size, num_boxes, 4).\n",
    "            input_masks (torch.Tensor, optional): Masks as input prompts, of shape (batch_size, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary segmentation logits of shape (batch_size, 1, height, width).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.sam_model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_points=input_points\n",
    "        )\n",
    "        return outputs\n",
    "        # outputs[\"iou_scores\"]\n",
    "        # logits = self.final_conv(outputs[\"pred_masks\"][:, 0, :, :, :])\n",
    "        # return {\"pred_masks\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3f8ab7-f5ce-4df3-858d-3e3453ceadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(train_loader, model, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop for a model that processes image, mask, and prompt data from a train loader.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        model (torch.nn.Module): Model to train.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to use for training ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Resize and Normalize transformations\n",
    "    # Explicit normalization for RGB and hyperspectral bands\n",
    "    # normalize_img = Normalize(\n",
    "    #     mean=[0.5, 0.485, 0.456, 0.406] + [0.5] * 8,  # RGB + Hyperspectral\n",
    "    #     std=[0.5, 0.229, 0.224, 0.225] + [0.5] * 8   # RGB + Hyperspectral\n",
    "    # )\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, (prompt, img, mask) in progress_bar:\n",
    "        # Preprocess the image\n",
    "        # img = normalize_img(img)  # Normalize input images\n",
    "        img = img.to(device)  # Move to the correct device\n",
    "        # print(torch.min(img), torch.max(img))\n",
    "        # Preprocess the mask\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Preprocess input points\n",
    "        random_point_x, random_point_y = prompt['centroid']\n",
    "        random_point = torch.stack((random_point_x, random_point_y), dim=-1).to(device)  # Combine and move to device\n",
    "        random_point = random_point.unsqueeze(1).unsqueeze(2).to(device)  # Shape: (batch_size, 1, 1, 2)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(pixel_values=img, input_points=random_point)\n",
    "\n",
    "        # Resize mask to match the predictions' spatial dimensions\n",
    "        predictions_shape = predictions[\"pred_masks\"].shape[-2:]  # (height, width)\n",
    "        resize_mask = Resize(predictions_shape, antialias=True)  # Dynamically adjust mask size\n",
    "        mask = resize_mask(mask)\n",
    "        mask = (mask > 0.5).float()\n",
    "        # print(mask)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions[\"pred_masks\"], mask.unsqueeze(1).unsqueeze(1).float())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss over all batches\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training completed. Average Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ccd388-fb20-4b91-ae10-55be0f5736ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SamModel were not initialized from the model checkpoint at facebook/sam-vit-base and are newly initialized because the shapes did not match:\n",
      "- mask_decoder.iou_prediction_head.proj_out.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- mask_decoder.iou_prediction_head.proj_out.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- mask_decoder.mask_tokens.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([13, 64]) in the model instantiated\n",
      "- vision_encoder.patch_embed.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 12, 16, 16]) in the model instantiated\n",
      "- vision_encoder.pos_embed: found shape torch.Size([1, 64, 64, 768]) in the checkpoint and torch.Size([1, 7, 7, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HyperspectralSAM(num_input_channels=12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d4f66e9-0134-4135-b15f-7c287b4273fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.7):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Apply sigmoid to logits with clamping for stability\n",
    "        probs = torch.sigmoid(logits).clamp(min=1e-6, max=1-1e-6)\n",
    "\n",
    "        # Flatten tensors for Dice and BCE calculations\n",
    "        probs_flat = probs.view(probs.size(0), -1)\n",
    "        targets_flat = targets.view(targets.size(0), -1)\n",
    "\n",
    "        # Weighted BCE loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        weight = torch.ones_like(targets) * 0.38  # Background weight\n",
    "        weight[targets == 1] = 1.0  # Foreground weight\n",
    "        bce_loss = (bce_loss * weight).mean()\n",
    "\n",
    "        # Dice loss with epsilon to avoid division by zero\n",
    "        intersection = (probs_flat * targets_flat).sum(dim=1)\n",
    "        dice_loss = 1 - (2.0 * intersection / (probs_flat.sum(dim=1) + targets_flat.sum(dim=1) + 1e-6)).mean()\n",
    "\n",
    "        # Combine BCE and Dice loss\n",
    "        total_loss = self.bce_weight * bce_loss + (1 - self.bce_weight) * dice_loss\n",
    "\n",
    "        # Validate loss for NaN\n",
    "        if not torch.isfinite(total_loss):\n",
    "            print(\"NaN detected in loss computation\")\n",
    "            print(f\"Logits: {logits}\")\n",
    "            print(f\"Targets: {targets}\")\n",
    "            print(f\"Probs: {probs}\")\n",
    "            print(f\"BCE Loss: {bce_loss}, Dice Loss: {dice_loss}\")\n",
    "            raise ValueError(\"Loss computation resulted in NaN\")\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f5908d-7a99-4ec4-9e61-df012d8f5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.28s/batch, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.4028641738601633\n",
      "Epoch 2/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.21s/batch, loss=0.191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.2254990820546408\n",
      "Epoch 3/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.23s/batch, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.20081608339741425\n",
      "Epoch 4/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.23s/batch, loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.19630842192752943\n",
      "Epoch 5/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.23s/batch, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.19491329346154188\n",
      "Epoch 6/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.20s/batch, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.19313984464954687\n",
      "Epoch 7/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.28s/batch, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.19169234242793676\n",
      "Epoch 8/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.21s/batch, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.19084249517402133\n",
      "Epoch 9/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.21s/batch, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1911328314123927\n",
      "Epoch 10/50, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:33<00:00,  1.27s/batch, loss=0.194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.18878419052910161\n",
      "Epoch 11/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:27<00:00,  1.19s/batch, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.18509201061081243\n",
      "Epoch 12/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:27<00:00,  1.18s/batch, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1846822528420268\n",
      "Epoch 13/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.23s/batch, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1844200985254468\n",
      "Epoch 14/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.18409485833064929\n",
      "Epoch 15/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.18431227472988335\n",
      "Epoch 16/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:28<00:00,  1.20s/batch, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1835605520251635\n",
      "Epoch 17/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.24s/batch, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.18315811133062518\n",
      "Epoch 18/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.23s/batch, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1831262562725995\n",
      "Epoch 19/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:33<00:00,  1.26s/batch, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.18301720957498294\n",
      "Epoch 20/50, Current LR: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.23s/batch, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.18325059699851112\n",
      "Epoch 21/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.23s/batch, loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.18058806416150686\n",
      "Epoch 22/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:28<00:00,  1.20s/batch, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17993737616249034\n",
      "Epoch 23/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:35<00:00,  1.29s/batch, loss=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17948512269838438\n",
      "Epoch 24/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.24s/batch, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17908597797960848\n",
      "Epoch 25/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:32<00:00,  1.25s/batch, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17885931077841166\n",
      "Epoch 26/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [02:07<00:00,  1.72s/batch, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17847177567514214\n",
      "Epoch 27/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.22s/batch, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17824003664222923\n",
      "Epoch 28/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:33<00:00,  1.26s/batch, loss=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17790134392074636\n",
      "Epoch 29/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.23s/batch, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17731785452043689\n",
      "Epoch 30/50, Current LR: [0.000125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.28s/batch, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17718866004331693\n",
      "Epoch 31/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:33<00:00,  1.26s/batch, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17548460855677323\n",
      "Epoch 32/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:32<00:00,  1.25s/batch, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17420577902246165\n",
      "Epoch 33/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:29<00:00,  1.21s/batch, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17402407889430588\n",
      "Epoch 34/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:35<00:00,  1.29s/batch, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1732252276427037\n",
      "Epoch 35/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:33<00:00,  1.26s/batch, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17273326802092628\n",
      "Epoch 36/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:32<00:00,  1.25s/batch, loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17216884807960406\n",
      "Epoch 37/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:33<00:00,  1.27s/batch, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1717123167740332\n",
      "Epoch 38/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:31<00:00,  1.24s/batch, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17098433121636109\n",
      "Epoch 39/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:33<00:00,  1.26s/batch, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17065903967296756\n",
      "Epoch 40/50, Current LR: [6.25e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.17023863039306691\n",
      "Epoch 41/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:35<00:00,  1.29s/batch, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1683791761060019\n",
      "Epoch 42/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:30<00:00,  1.22s/batch, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.16712380945682526\n",
      "Epoch 43/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:34<00:00,  1.28s/batch, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1663636289335586\n",
      "Epoch 44/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:40<00:00,  1.36s/batch, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.16586488304105965\n",
      "Epoch 45/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:28<00:00,  1.20s/batch, loss=0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.16516890739266937\n",
      "Epoch 46/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:18<00:00,  1.06s/batch, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.16459156613092166\n",
      "Epoch 47/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:17<00:00,  1.05s/batch, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.16425005809680834\n",
      "Epoch 48/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.16345207977133827\n",
      "Epoch 49/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.04s/batch, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1630487486317351\n",
      "Epoch 50/50, Current LR: [3.125e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 74/74 [01:16<00:00,  1.03s/batch, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.16237938786680634\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "experiment_name = \"centroid_prompt\"\n",
    "epochs = 50\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = DiceBCELoss()  # For binary segmentation masks\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Scheduler to halve the learning rate every 5 epochs\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Current LR: {scheduler.get_last_lr()}\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(train_loader, model, optimizer, criterion, device)\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    save_dir = f\"models/{experiment_name}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = f\"{save_dir}/hyperspectral_sam_epoch_{epoch+1}.pt\"\n",
    "    torch.save(model, model_path)\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35047639-5b36-4dea-b216-67c83aca2e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19850e4-c83f-43d9-92fe-84b5ed4f3f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7116d-6bd0-4f80-b008-203426a70933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
