{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071ec22a-1924-400a-838e-4f701230451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import SamModel, SamConfig\n",
    "\n",
    "class HyperspectralSAM(nn.Module):\n",
    "    def __init__(self, sam_checkpoint=\"facebook/sam-vit-base\", num_input_channels=12):\n",
    "        \"\"\"\n",
    "        Adapt SAM for hyperspectral data by modifying the input layer to handle more channels\n",
    "        and adding a final layer for binary segmentation.\n",
    "\n",
    "        Args:\n",
    "            sam_checkpoint (str): Hugging Face SAM model checkpoint.\n",
    "            num_input_channels (int): Number of input channels for hyperspectral data.\n",
    "        \"\"\"\n",
    "        super(HyperspectralSAM, self).__init__()\n",
    "\n",
    "        # Step 1: Load SAM configuration and update for hyperspectral channels\n",
    "        config = SamConfig.from_pretrained(sam_checkpoint)\n",
    "        config.vision_config.num_channels = num_input_channels\n",
    "\n",
    "        # Step 2: Load the SAM model with the updated configuration\n",
    "        self.sam_model = SamModel.from_pretrained(sam_checkpoint, config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "        # Step 3: Modify the vision encoder's first convolutional layer\n",
    "        original_conv = self.sam_model.vision_encoder.patch_embed.projection\n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=num_input_channels,\n",
    "            out_channels=original_conv.out_channels,\n",
    "            kernel_size=original_conv.kernel_size,\n",
    "            stride=original_conv.stride,\n",
    "            padding=original_conv.padding,\n",
    "            bias=original_conv.bias is not None\n",
    "        )\n",
    "        self.sam_model.vision_encoder.patch_embed.projection = new_conv\n",
    "\n",
    "        # Step 4: Add a final convolutional layer for binary segmentation\n",
    "        self.final_conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=1,  # Binary output\n",
    "            kernel_size=1  # 1x1 convolution for channel reduction\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, input_points=None, input_boxes=None, input_masks=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the adapted SAM model.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (torch.Tensor): Input tensor of shape (batch_size, num_channels, height, width).\n",
    "            input_points (torch.Tensor, optional): Points as input prompts, of shape (batch_size, num_points, 2).\n",
    "            input_boxes (torch.Tensor, optional): Boxes as input prompts, of shape (batch_size, num_boxes, 4).\n",
    "            input_masks (torch.Tensor, optional): Masks as input prompts, of shape (batch_size, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary segmentation logits of shape (batch_size, 1, height, width).\n",
    "        \"\"\"\n",
    "        outputs = self.sam_model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_points=input_points,\n",
    "            input_boxes=input_boxes,\n",
    "            input_masks=input_masks\n",
    "        )\n",
    "\n",
    "        # Apply the final convolutional layer to the predicted masks\n",
    "        logits = self.final_conv(outputs[\"pred_masks\"][:, 0, :, :, :])\n",
    "        return {\"pred_masks\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67235a91-733e-42a5-8d86-212378ef1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "class HyperspectralExpandedDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the expanded dataset.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        \"\"\"\n",
    "        Scans the directory structure to find all saved samples.\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing file paths for each sample.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for sample_name in os.listdir(self.root_dir):\n",
    "            sample_path = os.path.join(self.root_dir, sample_name)\n",
    "            if not os.path.isdir(sample_path):\n",
    "                continue\n",
    "\n",
    "            # Collect file paths for bands, binary mask, and prompt\n",
    "            bands_path = os.path.join(sample_path, \"bands.pt\")\n",
    "            mask_path = os.path.join(sample_path, \"binary_mask.tif\")\n",
    "            prompt_path = os.path.join(sample_path, \"prompt.json\")\n",
    "\n",
    "            if os.path.exists(bands_path) and os.path.exists(mask_path) and os.path.exists(prompt_path):\n",
    "                samples.append({\n",
    "                    \"bands\": bands_path,\n",
    "                    \"mask\": mask_path,\n",
    "                    \"prompt\": prompt_path\n",
    "                })\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads a sample.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (prompt, bands, binary_mask)\n",
    "        \"\"\"\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        # Load bands tensor\n",
    "        bands = None\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.load.*\")\n",
    "            bands = torch.load(sample[\"bands\"])\n",
    "\n",
    "        # Load binary mask as tensor\n",
    "        binary_mask = to_tensor(Image.open(sample[\"mask\"])).squeeze(0)  # Remove channel dimension\n",
    "\n",
    "        # Load prompt as a dictionary\n",
    "        with open(sample[\"prompt\"], \"r\") as f:\n",
    "            prompt = json.load(f)\n",
    "\n",
    "        return prompt, bands, binary_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eed851a-2e3d-4c43-9498-2dfb49f59d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamConfig\n",
    "\n",
    "# Initialize dataset and split into training and evaluation sets\n",
    "root_dir = \"./expanded_dataset_output\"  # Replace with the path to your dataset\n",
    "dataset = HyperspectralExpandedDataset(root_dir=root_dir)\n",
    "\n",
    "# Split dataset: 90% training, 10% evaluation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d67f67e-1f7e-46e3-a81e-0fc42d01bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10505\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73117c52-9e00-47bd-820f-f4c88fc7c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'centroid': [tensor([15.4310, 40.5926], dtype=torch.float64), tensor([108.8151, 117.7778], dtype=torch.float64)], 'random_point': [tensor([18, 39]), tensor([114, 119])]}\n",
      "torch.Size([2, 12, 120, 120])\n",
      "torch.Size([2, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "for (prompt, img, mask) in train_loader:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3f8ab7-f5ce-4df3-858d-3e3453ceadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(train_loader, model, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop for a model that processes image, mask, and prompt data from a train loader.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        model (torch.nn.Module): Model to train.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to use for training ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Resize transformations\n",
    "    resize_img = Resize((1024, 1024))  # Resize input images to 1024x1024\n",
    "\n",
    "    # Use tqdm for progress tracking\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, (prompt, img, mask) in progress_bar:\n",
    "        # Resize images\n",
    "        img = resize_img(img)  # Resize input images to 1024x1024\n",
    "\n",
    "        # Zip the random_point on the fly\n",
    "        random_point_x, random_point_y = prompt['random_point']\n",
    "        random_point = torch.stack((random_point_x, random_point_y), dim=-1)\n",
    "\n",
    "        # Validate batch size and points per image\n",
    "        batch_size = img.shape[0]\n",
    "        nb_points_per_image = random_point.shape[0] // batch_size\n",
    "        if nb_points_per_image * batch_size != random_point.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"Inconsistent shapes: batch_size={batch_size}, nb_points_per_image={nb_points_per_image}, \"\n",
    "                f\"random_point total size={random_point.shape[0]}\"\n",
    "            )\n",
    "\n",
    "        # Reshape to [batch_size, point_batch_size, nb_points_per_image, 2]\n",
    "        random_point = random_point.view(batch_size, 1, nb_points_per_image, 2)\n",
    "\n",
    "        # Ensure inputs are on the correct device\n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        random_point = random_point.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(img, input_points=random_point)  # Pass as 'input_points'\n",
    "\n",
    "        # Resize mask to match prediction size\n",
    "        predictions_shape = predictions[\"pred_masks\"].shape[-2:]  # (height, width)\n",
    "        resize_mask = Resize(predictions_shape)  # Dynamically adjust mask size\n",
    "        mask = resize_mask(mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions[\"pred_masks\"], mask.unsqueeze(1).float())  # Ensure mask is long (int) dtype\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update the progress bar with current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss over all batches\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training completed. Average Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f823734-c797-426d-86a9-58f9176d6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def evaluate(model, eval_loader, criterion, device, num_samples=4):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation dataset and visualize random samples.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        eval_loader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the evaluation on.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "\n",
    "    Returns:\n",
    "        float: Average evaluation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "\n",
    "    predictions = []\n",
    "    real_masks = []\n",
    "    inputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for prompts, bands, binary_masks in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "            # Resize inputs to 1024x1024\n",
    "            resize_img = Resize((1024, 1024))\n",
    "            bands = resize_img(bands).to(device)\n",
    "            binary_masks = binary_masks.to(device)\n",
    "\n",
    "            # Extract random points from the prompts\n",
    "            random_point_x, random_point_y = prompts['random_point']\n",
    "            random_point = torch.stack((random_point_x, random_point_y), dim=-1)\n",
    "            batch_size = bands.shape[0]\n",
    "            nb_points_per_image = random_point.shape[0] // batch_size\n",
    "            random_point = random_point.view(batch_size, 1, nb_points_per_image, 2).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(bands, input_points=random_point)\n",
    "            logits = outputs[\"pred_masks\"]\n",
    "\n",
    "            # Resize mask to match prediction size\n",
    "            predictions_shape = logits.shape[-2:]\n",
    "            resize_mask = Resize(predictions_shape)\n",
    "            binary_masks = resize_mask(binary_masks)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(logits, binary_masks.unsqueeze(1).float())\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and corresponding inputs\n",
    "            preds = (torch.sigmoid(logits) > 0.5).cpu()  # Threshold at 0.5 for binary masks\n",
    "            predictions.extend(preds)\n",
    "            real_masks.extend(binary_masks.cpu())\n",
    "            inputs.extend(bands.cpu())\n",
    "\n",
    "            # Stop collecting once we have enough samples\n",
    "            # if len(predictions) >= num_samples:\n",
    "            #     break\n",
    "\n",
    "    eval_loss /= len(eval_loader)\n",
    "    print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "\n",
    "    # Visualize the first few samples\n",
    "    # visualize_samples(inputs[:num_samples], predictions[:num_samples], real_masks[:num_samples])\n",
    "    return eval_loss\n",
    "\n",
    "\n",
    "def visualize_samples(inputs, predictions, real_masks):\n",
    "    \"\"\"\n",
    "    Visualize random samples with predictions and ground truth masks.\n",
    "\n",
    "    Args:\n",
    "        inputs (list): List of input images (bands).\n",
    "        predictions (list): List of predicted masks.\n",
    "        real_masks (list): List of ground truth masks.\n",
    "    \"\"\"\n",
    "    num_samples = len(inputs)\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]  # Ensure axes are iterable for a single sample\n",
    "\n",
    "    for i, (input_tensor, pred_mask, real_mask) in enumerate(zip(inputs, predictions, real_masks)):\n",
    "        ax_input, ax_pred, ax_real = axes[i]\n",
    "\n",
    "        # Display the input image (mean across channels)\n",
    "        input_image = to_pil_image(input_tensor[1:4])  # Convert to grayscale for visualization\n",
    "        ax_input.imshow(input_image)\n",
    "        ax_input.set_title(\"Input Image\")\n",
    "        ax_input.axis(\"off\")\n",
    "\n",
    "        # Display the predicted mask\n",
    "        ax_pred.imshow(pred_mask.squeeze(0), cmap=\"gray\")\n",
    "        ax_pred.set_title(\"Predicted Mask\")\n",
    "        ax_pred.axis(\"off\")\n",
    "\n",
    "        # Display the real mask\n",
    "        ax_real.imshow(real_mask.squeeze(0), cmap=\"gray\")\n",
    "        ax_real.set_title(\"Real Mask\")\n",
    "        ax_real.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ccd388-fb20-4b91-ae10-55be0f5736ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SamModel were not initialized from the model checkpoint at facebook/sam-vit-base and are newly initialized because the shapes did not match:\n",
      "- vision_encoder.patch_embed.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 12, 16, 16]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HyperspectralSAM(num_input_channels=12).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary segmentation masks\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fd6e76-8adf-408b-91ba-bf8167714e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6287/3114720594.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "last_successful_saved_epoch = 0\n",
    "model_path = f\"models/hyperspectral_sam_epoch_{last_successful_saved_epoch}.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f28159-7383-4c2a-b68f-88f271d941f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prompts, bands, binary_masks in eval_loader:\n",
    "#     model.eval()\n",
    "#     resize_img = Resize((1024, 1024))\n",
    "#     bands = resize_img(bands).to(device)\n",
    "#     binary_masks = binary_masks.to(device)\n",
    "#     # Extract random points from the prompts\n",
    "#     random_point_x, random_point_y = prompts['random_point']\n",
    "#     random_point = torch.stack((random_point_x, random_point_y), dim=-1)\n",
    "#     batch_size = bands.shape[0]\n",
    "#     nb_points_per_image = random_point.shape[0] // batch_size\n",
    "#     random_point = random_point.view(batch_size, 1, nb_points_per_image, 2).to(device)\n",
    "\n",
    "#     # Forward pass\n",
    "#     outputs = model(bands, input_points=random_point)\n",
    "#     logits = outputs[\"pred_masks\"]\n",
    "\n",
    "#     # Resize mask to match prediction size\n",
    "#     predictions_shape = logits.shape[-2:]\n",
    "#     resize_mask = Resize(predictions_shape)\n",
    "#     binary_masks = resize_mask(binary_masks)\n",
    "\n",
    "#     preds = (torch.sigmoid(logits) > 0.5).cpu()  # Threshold at 0.5 for binary masks\n",
    "    \n",
    "#     visualize_samples(bands.cpu(), preds.cpu(), binary_masks.cpu())\n",
    "#     model.train()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5908d-7a99-4ec4-9e61-df012d8f5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4727/4727 [51:54<00:00,  1.52batch/s, loss=0.263] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.39481552141758564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 2192/4727 [23:00<26:43,  1.58batch/s, loss=0.247] "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_model(train_loader, model, optimizer, criterion, device)\n",
    "    model_path = f\"models/hyperspectral_sam_epoch_{last_successful_saved_epoch+epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
