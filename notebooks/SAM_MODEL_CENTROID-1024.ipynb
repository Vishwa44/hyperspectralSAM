{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89920070-c7c6-4550-bd95-e01a299e6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    SamVisionConfig,\n",
    "    SamPromptEncoderConfig,\n",
    "    SamMaskDecoderConfig,\n",
    "    SamModel,\n",
    "    SamProcessor,\n",
    "    SamImageProcessor\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047820e6-5b10-4818-b5af-052a8eeae8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_per_image(img):\n",
    "    \"\"\"\n",
    "    Normalize an image tensor by dividing each pixel value by the maximum value in the image (plus 1).\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor): Image tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized image tensor with values in the range [0, 1].\n",
    "    \"\"\"\n",
    "    # Compute the maximum value per image\n",
    "    max_val = img.amax(dim=(-1, -2), keepdim=True)  # Max value per channel\n",
    "    max_val = max_val + 1  # Add 1 to avoid division by zero\n",
    "\n",
    "    # Normalize by max value\n",
    "    normalized_img = img / max_val\n",
    "    return normalized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67235a91-733e-42a5-8d86-212378ef1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralExpandedDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing the expanded dataset.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        \"\"\"\n",
    "        Scans the directory structure to find all saved samples.\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing file paths for each sample.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for sample_name in os.listdir(self.root_dir):\n",
    "            sample_path = os.path.join(self.root_dir, sample_name)\n",
    "            if not os.path.isdir(sample_path):\n",
    "                continue\n",
    "\n",
    "            # Collect file paths for bands, binary mask, and prompt\n",
    "            bands_path = os.path.join(sample_path, \"bands.pt\")\n",
    "            mask_path = os.path.join(sample_path, \"binary_mask.tif\")\n",
    "            prompt_path = os.path.join(sample_path, \"prompt.json\")\n",
    "\n",
    "            if os.path.exists(bands_path) and os.path.exists(mask_path) and os.path.exists(prompt_path):\n",
    "                samples.append({\n",
    "                    \"bands\": bands_path,\n",
    "                    \"mask\": mask_path,\n",
    "                    \"prompt\": prompt_path\n",
    "                })\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads a sample.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (prompt, bands, binary_mask)\n",
    "        \"\"\"\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        bands = None\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*torch.load.*\")\n",
    "            bands = torch.load(sample[\"bands\"])\n",
    "\n",
    "        binary_mask = to_tensor(Image.open(sample[\"mask\"])).squeeze(0)  # Remove channel dimension\n",
    "\n",
    "        with open(sample[\"prompt\"], \"r\") as f:\n",
    "            prompt = json.load(f)\n",
    "\n",
    "        return prompt, normalize_per_image(bands), binary_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eed851a-2e3d-4c43-9498-2dfb49f59d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./expanded_dataset_output\"\n",
    "dataset = HyperspectralExpandedDataset(root_dir=root_dir)\n",
    "\n",
    "# Split dataset: 90% training, 10% evaluation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d67f67e-1f7e-46e3-a81e-0fc42d01bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10505\n",
      "{'centroid': [56.394591887831744, 92.83508596227675], 'random_point': [3, 76]}\n",
      "torch.Size([12, 120, 120])\n",
      "torch.Size([120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "for (prompt, img, mask) in train_dataset:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73117c52-9e00-47bd-820f-f4c88fc7c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'centroid': [tensor([ 80.1880,  88.9929,  37.1098, 114.1507], dtype=torch.float64), tensor([ 16.4165,   6.5786,  14.8592, 116.4247], dtype=torch.float64)], 'random_point': [tensor([ 84,  82,  40, 117]), tensor([ 26,   3,   2, 114])]}\n",
      "torch.Size([4, 12, 120, 120])\n",
      "torch.Size([4, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "for (prompt, img, mask) in train_loader:\n",
    "    print(prompt)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071ec22a-1924-400a-838e-4f701230451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperspectralSAM(nn.Module):\n",
    "    def __init__(self, sam_checkpoint=\"facebook/sam-vit-base\", num_input_channels=12):\n",
    "        \"\"\"\n",
    "        Adapt SAM for hyperspectral data by modifying the input layer to handle more channels\n",
    "        and adding a final layer for binary segmentation.\n",
    "\n",
    "        Args:\n",
    "            sam_checkpoint (str): Hugging Face SAM model checkpoint.\n",
    "            num_input_channels (int): Number of input channels for hyperspectral data.\n",
    "        \"\"\"\n",
    "        super(HyperspectralSAM, self).__init__()\n",
    "\n",
    "        vision_config = SamVisionConfig(num_channels=12, image_size=1024)\n",
    "        decoder_config = SamMaskDecoderConfig(num_multimask_outputs = 1)\n",
    "        prompt_config  = SamPromptEncoderConfig(image_size=1024)\n",
    "        \n",
    "        config = SamConfig(vision_config = vision_config, \n",
    "                           prompt_encoder_config = prompt_config, \n",
    "                           mask_decoder_config = decoder_config, \n",
    "                           name_or_path=sam_checkpoint\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # self.processor = SamProcessor(img_processor)\n",
    "        self.sam_model = SamModel.from_pretrained(sam_checkpoint, config=config, ignore_mismatched_sizes=True)\n",
    "        self.sam_model.train()\n",
    "    def forward(self, pixel_values, input_points=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the adapted SAM model.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (torch.Tensor): Input tensor of shape (batch_size, num_channels, height, width).\n",
    "            input_points (torch.Tensor, optional): Points as input prompts, of shape (batch_size, num_points, 2).\n",
    "            input_boxes (torch.Tensor, optional): Boxes as input prompts, of shape (batch_size, num_boxes, 4).\n",
    "            input_masks (torch.Tensor, optional): Masks as input prompts, of shape (batch_size, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary segmentation logits of shape (batch_size, 1, height, width).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.sam_model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_points=input_points\n",
    "        )\n",
    "        return outputs\n",
    "        # outputs[\"iou_scores\"]\n",
    "        # logits = self.final_conv(outputs[\"pred_masks\"][:, 0, :, :, :])\n",
    "        # return {\"pred_masks\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3f8ab7-f5ce-4df3-858d-3e3453ceadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(train_loader, model, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Training loop for a model that processes image, mask, and prompt data from a train loader.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        model (torch.nn.Module): Model to train.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to use for training ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Resize and Normalize transformations\n",
    "    # Explicit normalization for RGB and hyperspectral bands\n",
    "    # normalize_img = Normalize(\n",
    "    #     mean=[0.5, 0.485, 0.456, 0.406] + [0.5] * 8,  # RGB + Hyperspectral\n",
    "    #     std=[0.5, 0.229, 0.224, 0.225] + [0.5] * 8   # RGB + Hyperspectral\n",
    "    # )\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", unit=\"batch\")\n",
    "    resize_img = Resize((1024, 1024), antialias=True)  \n",
    "\n",
    "    for batch_idx, (prompt, img, mask) in progress_bar:\n",
    "        # Preprocess the image\n",
    "        # img = normalize_img(img)  # Normalize input images\n",
    "        img = resize_img(img).to(device)  # Move to the correct device\n",
    "        # print(torch.min(img), torch.max(img))\n",
    "        # Preprocess the mask\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Preprocess input points\n",
    "        random_point_x, random_point_y = prompt['centroid']\n",
    "        random_point = torch.stack((random_point_x, random_point_y), dim=-1).to(device)  # Combine and move to device\n",
    "        random_point = random_point.unsqueeze(1).unsqueeze(2).to(device)  # Shape: (batch_size, 1, 1, 2)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(pixel_values=img, input_points=random_point)\n",
    "\n",
    "        # Resize mask to match the predictions' spatial dimensions\n",
    "        predictions_shape = predictions[\"pred_masks\"].shape[-2:]  # (height, width)\n",
    "        resize_mask = Resize(predictions_shape, antialias=True)  # Dynamically adjust mask size\n",
    "        mask = resize_mask(mask)\n",
    "        mask = (mask > 0.5).float()\n",
    "        # print(mask)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions[\"pred_masks\"], mask.unsqueeze(1).unsqueeze(1).float())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss over all batches\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training completed. Average Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ccd388-fb20-4b91-ae10-55be0f5736ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avm6288/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SamModel were not initialized from the model checkpoint at facebook/sam-vit-base and are newly initialized because the shapes did not match:\n",
      "- mask_decoder.iou_prediction_head.proj_out.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- mask_decoder.iou_prediction_head.proj_out.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- mask_decoder.mask_tokens.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- vision_encoder.patch_embed.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 12, 16, 16]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HyperspectralSAM(num_input_channels=12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d4f66e9-0134-4135-b15f-7c287b4273fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.7):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Apply sigmoid to logits with clamping for stability\n",
    "        probs = torch.sigmoid(logits).clamp(min=1e-6, max=1-1e-6)\n",
    "\n",
    "        # Flatten tensors for Dice and BCE calculations\n",
    "        probs_flat = probs.view(probs.size(0), -1)\n",
    "        targets_flat = targets.view(targets.size(0), -1)\n",
    "\n",
    "        # Weighted BCE loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        weight = torch.ones_like(targets) * 0.38  # Background weight\n",
    "        weight[targets == 1] = 1.0  # Foreground weight\n",
    "        bce_loss = (bce_loss * weight).mean()\n",
    "\n",
    "        # Dice loss with epsilon to avoid division by zero\n",
    "        intersection = (probs_flat * targets_flat).sum(dim=1)\n",
    "        dice_loss = 1 - (2.0 * intersection / (probs_flat.sum(dim=1) + targets_flat.sum(dim=1) + 1e-6)).mean()\n",
    "\n",
    "        # Combine BCE and Dice loss\n",
    "        total_loss = self.bce_weight * bce_loss + (1 - self.bce_weight) * dice_loss\n",
    "\n",
    "        # Validate loss for NaN\n",
    "        if not torch.isfinite(total_loss):\n",
    "            print(\"NaN detected in loss computation\")\n",
    "            print(f\"Logits: {logits}\")\n",
    "            print(f\"Targets: {targets}\")\n",
    "            print(f\"Probs: {probs}\")\n",
    "            print(f\"BCE Loss: {bce_loss}, Dice Loss: {dice_loss}\")\n",
    "            raise ValueError(\"Loss computation resulted in NaN\")\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5908d-7a99-4ec4-9e61-df012d8f5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [49:57<00:00,  1.27s/batch, loss=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.2990678672547974\n",
      "Epoch 2/15, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [47:12<00:00,  1.20s/batch, loss=0.0856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.2171702542406855\n",
      "Epoch 3/15, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [47:12<00:00,  1.20s/batch, loss=0.226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.2091868589592817\n",
      "Epoch 4/15, Current LR: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [47:14<00:00,  1.20s/batch, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.2031842088153469\n",
      "Epoch 5/15, Current LR: [0.0004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [47:11<00:00,  1.20s/batch, loss=0.164] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1993908326294393\n",
      "Epoch 6/15, Current LR: [0.0004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [47:07<00:00,  1.20s/batch, loss=0.38]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.19399780560719784\n",
      "Epoch 7/15, Current LR: [0.0004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [47:11<00:00,  1.20s/batch, loss=0.156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.1945723706727151\n",
      "Epoch 8/15, Current LR: [0.0004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [47:12<00:00,  1.20s/batch, loss=0.213] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.19181925751892898\n",
      "Epoch 9/15, Current LR: [0.00032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2364/2364 [47:09<00:00,  1.20s/batch, loss=0.168] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Average Loss: 0.186587841243699\n",
      "Epoch 10/15, Current LR: [0.00032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 1810/2364 [36:06<11:00,  1.19s/batch, loss=0.222] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "experiment_name = \"centroid_prompt_1024\"\n",
    "epochs = 15\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = DiceBCELoss()  # For binary segmentation masks\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Scheduler to halve the learning rate every 5 epochs\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.8)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Current LR: {scheduler.get_last_lr()}\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(train_loader, model, optimizer, criterion, device)\n",
    "    \n",
    "    # Save the model checkpoint\n",
    "    save_dir = f\"models/{experiment_name}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = f\"{save_dir}/hyperspectral_sam_epoch_{epoch+1}.pt\"\n",
    "    torch.save(model, model_path)\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35047639-5b36-4dea-b216-67c83aca2e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19850e4-c83f-43d9-92fe-84b5ed4f3f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7116d-6bd0-4f80-b008-203426a70933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
